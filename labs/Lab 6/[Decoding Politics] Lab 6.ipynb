{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Text as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we'll be talking about how we can use text as data. Social scientists are increasingly turning toward techniques will talk about this week and next week to learn something new or interesting about our world. Hopefully, I've motivated what we want to be able to accomplish enough because this lab won't be as entertaining as some of the findings in previous labs. This week's lab is entirely dedicated as a setup for next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PUT YOUR NAME HERE\n",
    "name = \"FirstName LastName\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nixon's Resignation Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't already know, President Richard Nixon resigned as the President of the United States in advance of an impending impeachment trial. We've found the text for you and we've done the gross stuff for you already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = open(\"./nixon_resignation.txt\", \"r\")\n",
    "speech = text_file.read()[110:] # when the speech actually starts\n",
    "speech = speech.replace('\\n',' ') \n",
    "speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1:** Using slicing, extract the first word of the speech (\"Good\") and store it in the variable `good`. \"Good\" starts at index 0 and ends at index 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good = ...\n",
    "good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert good == \"Good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2:** Using slicing and the `good` variable from 1.1, extract all the letter \"d\" in \"Good\" into the variable `goo`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goo = ...\n",
    "goo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert goo == \"Goo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3:** Using slicing, reverse the characters of `speech` into the variable `reverse`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse = ...\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert len(reverse) == len(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "## 2. Pre-processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language, in general, has a lot of technicalities that we want to remove so that it's easier to do some sort of analysis. Computers, as we have mentioned, are really stupid and can't interpret these rules without us programming it in, which can take a lot of time. Instead, we will process the text such that analysis will be simpler. We'll go through a few of the pre-processing techniques that are essential to most analyses in this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, some of the words have a punctuation mark attached to the end of it. Fundamentally, there is not a ton of difference between \"evening:\" and \"evening\" since it says more about grammatical rules than it does about content. \n",
    "\n",
    "**2.1:** Remove all `.`, `,`, `-` and `:` from the text by replacing any occurrences it with `\"\"`. There are many ways to approach this, but for now use the `replace` function. NOTE: You may add extra lines to accomplish this task (though you can technically just chain it together). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech = ...\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert \".\" not in speech\n",
    "assert \",\" not in speech\n",
    "assert \"-\" not in speech\n",
    "assert \":\" not in speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as there isn't a ton of difference between \"evening:\" and \"evening,\" there's also not a lot of difference among \"evening,\" \"EVENING,\" or \"Evening.\" In that case, we want to just treat everything in the same case. The function `lower` does this for you. We'll provide an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UPPER = 'GOOD'\n",
    "lower = UPPER.lower()\n",
    "lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2:** Change everything in `speech` to be lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech = ...\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert speech[0] != \"G\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Right now, this is a bunch of words and it's just really clunky. Let's split up all the words into a list so we can treat each word of this giant string \n",
    "\n",
    "**2.3:** Using the `split` function that we talked about in the demo, save all the words from `speech` into `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ...\n",
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert len(words) == 1799"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, there's a lot more pre-processing of text that we do before any sort of text analysis. But, it's usually fairly tedious and not a good use of time so we'll stop here with the pre-processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple tool for text analysis is generally word frequency tables. It's not often the greatest tool, but it's enough to illustrate basic textual analysis. In order to get there, we need to first build up our understanding of dictionaries, which we can do incrementally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**: Create an empty dictionary called `word_frequency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_frequency = ...\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert type(word_frequency) is dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2**: Add the word \"president\" into the `word_frequency` dictionary with its value as the int `2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "...\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert \"president\" in word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we wanted to look up the word \"president\" in `word_frequency` to get its frequency, we would do `word_frequency['president']`. This syntax is very similar to just adding the key-value pair into the dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3**: Now, let's build the `frequency` dictionary of President Nixon's resignation speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_frequency(text): \n",
    "    word_frequency = .. # create an empty dictionary here\n",
    "    for ...: \n",
    "        word = text[i]\n",
    "        if ... in ... : # check if the key is already in the dictionary\n",
    "            ...\n",
    "        else: # key isn't in the dictionary so this is the first time we encounter the word\n",
    "            ... \n",
    "    return ...\n",
    "\n",
    "word_frequency = build_word_frequency(words)\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert word_frequency['the'] == 130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chaotic order that it was displayed, it's hard to really learn anything from `word_frequency`. Let's sort it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_word_frequency = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should notice is that a lot of the most frequency used words don't tell us a lot. It's generally considered good practice to remove words such as `the`, `that`, `a`, etc. (these are called stopwords) in the pre-processing phase. We won't ask you to do that now, but that's something to consider in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without reading the actual text, the frequencies can tell us a few things about the speech. For example: does President Nixon express regret or does he apologize?\n",
    "\n",
    "**3.4:** Without reading the speech, how many times does President Nixon mention words like \"regret\", \"sorry\", \"wrongdoing\" or \"apologize\"? You should access these values using the `word_frequency` dictionary. Leave what you tried in the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's lab is short, but next week's material really builds upon what we covered in this week's lab. Make sure you understand and are comfortable with the fundamentals that we went over today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've reached the end of this lab! While this lab is graded by effort, we still want to make sure that all of you get a grade for this assignment. To submit, go to datahub.berkeley.edu. Find your file. Click the checkbox next to the file. If it is green, press shutdown. If it isn't lit up, press \"Download\". After you download it, please rename the file to follow this format, \"[YOUR NAME] WEEK 6 LAB.ipynb\", and submit it to the correct bCourses assignment page. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
